{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.caasimada.net/category/wararka/\"\n",
    "page_number = 1\n",
    "headlines = []\n",
    "article_contents = []\n",
    "\n",
    "while page_number <= 5:\n",
    "    # Create the URL for the current page\n",
    "    url = base_url + \"page/\" + str(page_number) + \"/\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the headline news elements on the page\n",
    "    headline_elements = soup.find_all('h3', class_='entry-title td-module-title')\n",
    "\n",
    "    # If no headline elements found, exit the loop\n",
    "    if not headline_elements:\n",
    "        break\n",
    "\n",
    "    for headline in headline_elements:\n",
    "        link_element = headline.find('a')\n",
    "        if link_element:\n",
    "            article_url = link_element['href']\n",
    "            article_response = requests.get(article_url)\n",
    "            article_soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "\n",
    "            article_paragraphs = article_soup.find_all('p')\n",
    "            article_text = ' '.join([p.get_text().strip() for p in article_paragraphs if 'title heading-typo' not in p.get('class', []) and 'bs--gdpr-low' not in p.parent.get('class', [])])\n",
    "            if article_text:\n",
    "                headlines.append(link_element.text.strip())\n",
    "                article_contents.append(article_text)\n",
    "\n",
    "    # Increment the page number for the next iteration\n",
    "    page_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "data = {'Headline': headlines, 'Content': article_contents}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv('caasimadda_news.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Headline  \\\n",
      "0  Muxuu R/wasaare ku-xigeenka kusoo arkay xerada...   \n",
      "1  Xil. Abiib oo laga gudbiyey dacwad culus iyo x...   \n",
      "2  Maxaan ka naqaanaa ninka Trump u noqonaya ku-x...   \n",
      "3  Galmudug oo sheegtay danta laga lahaa hubkii l...   \n",
      "4  Sidee ayey ku timid guusha uu ku faanay R/W Xa...   \n",
      "\n",
      "                                             Summary  \n",
      "0  Muqdisho (Caasimada Online) – Ra’iisul wasaare...  \n",
      "1  Maxamed Cabdi Afyare, oo ah sharci-yaqaan ayaa...  \n",
      "2  Markaas ka dib, Mr Vance ayaa bartiisa X ee ho...  \n",
      "3  Dhuusamareeb (Caasimada Online) –Dowlad Gobole...  \n",
      "4  Ra’iisul Wasaaraha oo shalay ka qeyb-galay mun...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')  # For tokenization\n",
    "nltk.download('wordnet')  # For lemmatization\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('caasimadda_news.csv')  # Adjust path accordingly\n",
    "# Assuming stopwords_list.csv has been loaded\n",
    "stopwords_df = pd.read_csv('stopwords_list.csv')\n",
    "stopwords = stopwords_df['words'].tolist()\n",
    "\n",
    "# Preprocessing function with your code integrated\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'(\\w+)\\.(\\w+)', r'\\1 \\2', text)  # Split words separated by periods\n",
    "    text = re.sub(r'(\\w+),(\\w+)', r'\\1 \\2', text)  # Split words separated by commas\n",
    "    text = re.sub(r'[^a-zA-Z\\s-]', '', text, re.I | re.A)\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('-', '')  # Remove dashes\n",
    "    text = text.strip()\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return ' '.join(tokens)  # Return a string for further processing\n",
    "\n",
    "# Sentence Tokenization and Preprocessing of Each Sentence\n",
    "def tokenize_and_preprocess_sentences(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    preprocessed_sentences = [preprocess(sentence) for sentence in sentences]\n",
    "    # Filter out any empty sentences\n",
    "    preprocessed_sentences = [sentence for sentence in preprocessed_sentences if sentence]\n",
    "    return sentences, preprocessed_sentences  # Return original and preprocessed sentences\n",
    "\n",
    "# Constructing the TextRank similarity matrix\n",
    "def build_similarity_matrix(preprocessed_sentences):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    if not preprocessed_sentences:\n",
    "        return np.zeros((0, 0))  # Return an empty matrix if there are no sentences\n",
    "    sentence_vectors = vectorizer.fit_transform(preprocessed_sentences).toarray()\n",
    "    similarity_matrix = cosine_similarity(sentence_vectors)\n",
    "    return similarity_matrix\n",
    "\n",
    "# TextRank Algorithm\n",
    "def text_rank(similarity_matrix):\n",
    "    if similarity_matrix.shape[0] == 0:\n",
    "        return np.array([])  # Return an empty array if the similarity matrix is empty\n",
    "    nx_graph = np.array(similarity_matrix)\n",
    "    scores = np.array([1] * len(similarity_matrix))  # Initial score for each sentence\n",
    "    beta = 0.85  # Damping factor\n",
    "    for _ in range(10):  # Iteration for convergence\n",
    "        scores_new = (1-beta) + beta * np.matmul(nx_graph, scores)\n",
    "        if np.allclose(scores, scores_new, atol=1e-6):\n",
    "            break\n",
    "        scores = scores_new\n",
    "    return scores\n",
    "\n",
    "# Generate summaries\n",
    "def generate_summary(content):\n",
    "    if not isinstance(content, str):\n",
    "        return ''  # Return an empty summary for invalid content\n",
    "    original_sentences, preprocessed_sentences = tokenize_and_preprocess_sentences(content)\n",
    "    if not preprocessed_sentences:\n",
    "        return ''  # Return an empty summary if there are no preprocessed sentences\n",
    "    similarity_matrix = build_similarity_matrix(preprocessed_sentences)\n",
    "    sentence_scores = text_rank(similarity_matrix)\n",
    "    if len(sentence_scores) == 0:\n",
    "        return ''  # Return an empty summary if there are no sentence scores\n",
    "    ranked_sentence_indices = np.argsort(-sentence_scores)  # Sort sentences by score\n",
    "    top_sentence_indices = ranked_sentence_indices[:3]  # Select top 3 sentences for summary\n",
    "    summary = ' '.join([original_sentences[index] for index in top_sentence_indices])\n",
    "    return summary\n",
    "\n",
    "# Applying the summarization\n",
    "df['Summary'] = df['Content'].apply(generate_summary)\n",
    "\n",
    "# Display the first few summaries\n",
    "print(df[['Headline', 'Summary']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('summarized1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
